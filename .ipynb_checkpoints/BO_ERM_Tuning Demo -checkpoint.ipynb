{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f492b3",
   "metadata": {},
   "source": [
    "https://github.com/ntienvu/KnownOptimum_BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c06174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesOpt_KnownOptimumValue,BayesOpt\n",
    "import numpy as np\n",
    "from bayes_opt import vis_ERM,functions\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd273f",
   "metadata": {},
   "source": [
    "https://pymoo.org/problems/single/ackley.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef5b34",
   "metadata": {},
   "source": [
    "## Black Box function - 2 D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07cd1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourFunction:\n",
    "    def __init__(self):\n",
    "       \n",
    "        # define the search range for each variable\n",
    "        self.bounds = np.asarray([\n",
    "                                [-5,10],  # variable 1\n",
    "                                  [0,15] # variable 2\n",
    "                                 ])\n",
    "            \n",
    "        self.input_dim = self.bounds.shape[0] # this is 2\n",
    "\n",
    "        # do we want to maximize the function or minimize ?\n",
    "        self.ismax=-1  # set -1 if we want to minimize\n",
    "        \n",
    "        # define the known optimum value if it is available\n",
    "        self.fstar = 0 # set it None if we dont know #0.397887*self.ismax\n",
    "        \n",
    "        # define the name of your function\n",
    "        self.name='ICML20'\n",
    "        \n",
    "    def evaluate_single_fx(self,X): # this is actually a Branin function\n",
    "        # evaluate y=f(X)\n",
    "        X = np.reshape(X,self.input_dim)\n",
    "        x1,x2=X[0],X[1]\n",
    "     \n",
    "        a=1\n",
    "        b=5.1/(4*np.pi**2)\n",
    "        c=5/np.pi\n",
    "        r=6\n",
    "        s=10\n",
    "        t=1/(8*np.pi)\n",
    "        y=a*(x2-b*x1*x1+c*x1-r)**2+s*(1-t)*np.cos(x1)+s    \n",
    "      \n",
    "        return y*self.ismax # return the (-1) * fx for minimization problem\n",
    "   \n",
    "    \n",
    "    def func(self,X):\n",
    "        X=np.asarray(X)        \n",
    "       \n",
    "        if len(X.shape)==1: # 1 data point\n",
    "            fx=self.evaluate_single_fx(X)\n",
    "        else: # multiple data points\n",
    "            fx=np.apply_along_axis( self.evaluate_single_fx,1,X)\n",
    "            \n",
    "        return fx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f068a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunction=YourFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc0933",
   "metadata": {},
   "source": [
    "### Normal GP with EI acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e006366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-308.12909601]\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     0  [ 6.997 13.929]         -179.592               -179.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.Y=(self.Y_ori-np.mean(self.Y_ori))/np.std(self.Y_ori)\n",
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:223: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  fstar_scaled=(self.fstar-np.mean(self.Y_ori))/np.std(self.Y_ori)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_lcb=[[-3.50804098]] y_ucb=[[2.41097461]] fstar_scaled=3.7944\n",
      "EI\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     1  [ 3.677 13.674]         -140.511               -140.511\n",
      "y_lcb=[[-4.09626199]] y_ucb=[[2.60445593]] fstar_scaled=2.9245\n",
      "EI\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     2  [ 4.533 10.446]            -89.4                  -89.4\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     3  [ 4.452 10.561]          -90.082                  -89.4\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     4  [ 4.621 10.193]          -86.373                -86.373\n",
      "estimated lengthscale [0.22722682]\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     5  [3.803 9.399]          -59.928                -59.928\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     6  [3.243 9.059]          -47.542                -47.542\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     7  [2.29  8.656]          -35.298                -35.298\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     8  [1.865 8.595]           -33.37                 -33.37\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     9  [1.679 8.775]          -34.802                 -33.37\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    10  [1.532 7.977]          -27.279                -27.279\n",
      "estimated lengthscale [0.12589764]\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    11  [1.522 6.902]          -19.617                -19.617\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    12  [1.459 6.514]          -17.631                -17.631\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    13  [0.983 5.8  ]          -16.862                -16.862\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    14  [1.59  5.656]          -13.273                -13.273\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    15  [1.894 5.018]           -9.411                 -9.411\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    16  [2.395 4.266]           -4.737                 -4.737\n",
      "estimated lengthscale [0.12579955]\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    17  [2.982 3.593]           -1.936                 -1.936\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    18  [3.397 3.249]           -2.067                 -1.936\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    19  [3.011 3.143]           -1.063                 -1.063\n"
     ]
    }
   ],
   "source": [
    "init_X = np.asarray( [[-5,0]])\n",
    "init_Y=myfunction.func(init_X)\n",
    "print(init_Y)\n",
    "\n",
    "# create an empty object for BO using transformed GP\n",
    "acq_name='erm'\n",
    "IsTGP=0 # using Transformed GP\n",
    "\n",
    "bo_tgp=BayesOpt_KnownOptimumValue(myfunction.func,myfunction.bounds,fstar=myfunction.fstar, \\\n",
    "                              acq_name=acq_name,IsTGP=IsTGP,verbose=1)\n",
    "bo_tgp.init_with_data(init_X=init_X,init_Y=init_Y)\n",
    "\n",
    "#vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar)\n",
    "\n",
    "\n",
    "NN=10*myfunction.input_dim\n",
    "for index in range(0,NN):\n",
    "    xt=bo_tgp.select_next_point()\n",
    "    #print(bo_tgp.X_ori[-1])\n",
    "    print(tabulate([[ index,np.round(bo_tgp.X_ori[-1],3), np.round(bo_tgp.Y_ori[-1],3), np.round(bo_tgp.Y_ori.max(),3)]], \\\n",
    "               headers=['Iter','Selected x', 'Output y=f(x)', 'Best Observed Value']))\n",
    "    #vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar) #一維圖-->二維故不能畫\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697162b9",
   "metadata": {},
   "source": [
    "### Transformed GP with EI acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ac1550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-308.12909601]\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     0  [ 3.637 10.227]          -70.551                -70.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.Y=(self.Y_ori-np.mean(self.Y_ori))/np.std(self.Y_ori)\n",
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:223: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  fstar_scaled=(self.fstar-np.mean(self.Y_ori))/np.std(self.Y_ori)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     1  [ 7.106 10.958]         -111.482                -70.551\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     2  [ 3.927 13.795]          -148.48                -70.551\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     3  [5.284 7.173]          -50.906                -50.906\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     4  [1.676 6.772]          -18.459                -18.459\n",
      "estimated lengthscale [0.23679111]\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     5  [3.193 3.035]           -1.051                 -1.051\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     6  [7.676 1.889]          -11.944                 -1.051\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     7  [-2.547  9.92 ]           -2.989                 -1.051\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     8  [-5.    14.635]          -19.238                 -1.051\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     9  [-5.     5.599]         -147.003                 -1.051\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "    10  [10.     6.141]           -11.79                 -1.051\n",
      "estimated lengthscale [0.20594417]\n",
      "  Iter  Selected x      Output y=f(x)    Best Observed Value\n",
      "------  ------------  ---------------  ---------------------\n",
      "    11  [10. 15.]            -145.872                 -1.051\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    12  [0.137 0.   ]           -52.98                 -1.051\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "    13  [-0.643 15.   ]          -80.446                 -1.051\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "    14  [-1.147  3.873]           -30.94                 -1.051\n",
      "  Iter  Selected x      Output y=f(x)    Best Observed Value\n",
      "------  ------------  ---------------  ---------------------\n",
      "    15  [10.  0.]             -10.961                 -1.051\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    16  [4.578 0.   ]          -10.731                 -1.051\n",
      "estimated lengthscale [0.18207589]\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    17  [10.    9.67]          -46.398                 -1.051\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "    18  [ 0.44  11.568]          -57.671                 -1.051\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "    19  [-5.     8.992]          -79.895                 -1.051\n"
     ]
    }
   ],
   "source": [
    "init_X = np.asarray( [[-5,0]])\n",
    "init_Y=myfunction.func(init_X)\n",
    "print(init_Y)\n",
    "\n",
    "# create an empty object for BO using transformed GP\n",
    "acq_name='ei'\n",
    "IsTGP=1 # using Transformed GP\n",
    "\n",
    "bo_tgp=BayesOpt_KnownOptimumValue(myfunction.func,myfunction.bounds,fstar=myfunction.fstar, \\\n",
    "                              acq_name=acq_name,IsTGP=IsTGP,verbose=1)\n",
    "bo_tgp.init_with_data(init_X=init_X,init_Y=init_Y)\n",
    "\n",
    "#vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar)\n",
    "\n",
    "\n",
    "NN=10*myfunction.input_dim\n",
    "for index in range(0,NN):\n",
    "    xt=bo_tgp.select_next_point()\n",
    "    #print(bo_tgp.X_ori[-1])\n",
    "    print(tabulate([[ index,np.round(bo_tgp.X_ori[-1],3), np.round(bo_tgp.Y_ori[-1],3), np.round(bo_tgp.Y_ori.max(),3)]], \\\n",
    "               headers=['Iter','Selected x', 'Output y=f(x)', 'Best Observed Value']))\n",
    "    #vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar) #一維圖-->二維故不能畫\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8683c47",
   "metadata": {},
   "source": [
    "### Transformed GP with erm acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb9078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-308.12909601]\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     0  [ 8.204 11.916]         -112.347               -112.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.Y=(self.Y_ori-np.mean(self.Y_ori))/np.std(self.Y_ori)\n",
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:223: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  fstar_scaled=(self.fstar-np.mean(self.Y_ori))/np.std(self.Y_ori)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_lcb=[[-2.8134303]] y_ucb=[[1.84974022]] fstar_scaled=2.1477\n",
      "EI\n",
      "  Iter  Selected x         Output y=f(x)    Best Observed Value\n",
      "------  ---------------  ---------------  ---------------------\n",
      "     1  [ 5.188 13.39 ]         -162.504               -112.347\n",
      "y_lcb=[[-3.78319238]] y_ucb=[[2.3371521]] fstar_scaled=2.3403\n",
      "EI\n",
      "  Iter  Selected x      Output y=f(x)    Best Observed Value\n",
      "------  ------------  ---------------  ---------------------\n",
      "     2  [6.33 9.15]           -84.373                -84.373\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     3  [6.621 9.492]          -89.059                -84.373\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     4  [5.935 8.647]          -75.919                -75.919\n",
      "estimated lengthscale [0.18067549]\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     5  [5.244 7.61 ]          -55.875                -55.875\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     6  [4.754 6.716]          -39.154                -39.154\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     7  [4.289 5.713]          -23.379                -23.379\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     8  [3.934 4.809]          -12.686                -12.686\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "     9  [3.63  3.875]           -5.322                 -5.322\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    10  [3.418 3.036]           -1.697                 -1.697\n",
      "estimated lengthscale [0.20940064]\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    11  [3.284 2.268]           -0.506                 -0.506\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    12  [3.283 1.955]           -0.539                 -0.506\n",
      "  Iter  Selected x      Output y=f(x)    Best Observed Value\n",
      "------  ------------  ---------------  ---------------------\n",
      "    13  [2.74 2.34]            -1.234                 -0.506\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    14  [3.14  2.064]           -0.443                 -0.443\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    15  [3.12  2.048]            -0.46                 -0.443\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    16  [3.116 2.338]           -0.403                 -0.403\n",
      "estimated lengthscale [0.22068938]\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    17  [3.13  2.285]           -0.399                 -0.399\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    18  [3.131 2.274]           -0.399                 -0.399\n",
      "  Iter  Selected x       Output y=f(x)    Best Observed Value\n",
      "------  -------------  ---------------  ---------------------\n",
      "    19  [3.131 2.273]           -0.399                 -0.399\n"
     ]
    }
   ],
   "source": [
    "init_X = np.asarray( [[-5,0]])\n",
    "init_Y=myfunction.func(init_X)\n",
    "print(init_Y)\n",
    "\n",
    "# create an empty object for BO using transformed GP\n",
    "acq_name='erm'\n",
    "IsTGP=1 # using Transformed GP\n",
    "\n",
    "bo_tgp=BayesOpt_KnownOptimumValue(myfunction.func,myfunction.bounds,fstar=myfunction.fstar, \\\n",
    "                              acq_name=acq_name,IsTGP=IsTGP,verbose=1)\n",
    "bo_tgp.init_with_data(init_X=init_X,init_Y=init_Y)\n",
    "\n",
    "#vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar)\n",
    "\n",
    "\n",
    "NN=10*myfunction.input_dim\n",
    "for index in range(0,NN):\n",
    "    xt=bo_tgp.select_next_point()\n",
    "    #print(bo_tgp.X_ori[-1])\n",
    "    print(tabulate([[ index,np.round(bo_tgp.X_ori[-1],3), np.round(bo_tgp.Y_ori[-1],3), np.round(bo_tgp.Y_ori.max(),3)]], \\\n",
    "               headers=['Iter','Selected x', 'Output y=f(x)', 'Best Observed Value']))\n",
    "    #vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar) #一維圖-->二維故不能畫\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574daf2",
   "metadata": {},
   "source": [
    "# Iris data Classification：Tuning xgboost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95279339",
   "metadata": {},
   "source": [
    "## No tuning max_depth & eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44c5c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "15 135\n",
      "0.8814814814814815\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 6,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'min_child_weight': 1,\n",
    "    'colsample_bytree':1,\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "model = xgb.train(param, dtrain)\n",
    "preds = model.predict(dtest)\n",
    "preds = np.asarray([np.argmax(line) for line in preds])\n",
    "print(len(X_train),len(X_test))\n",
    "print (accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e192a4",
   "metadata": {},
   "source": [
    "## Tuning ：20 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f7a1a",
   "metadata": {},
   "source": [
    "Random Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b34bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "for i in range(20):\n",
    "    max_depth = random.uniform(3.0,9.0)\n",
    "    eta = random.uniform(0.1,0.5)\n",
    "    min_child_weight = random.uniform(1,20)\n",
    "    colsample_bytree = random.uniform(0.1,1)\n",
    "    \n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    param = {\n",
    "    'max_depth': round(max_depth),  # the maximum depth of each tree\n",
    "    'eta': eta,  # the training step for each iteration\n",
    "    'min_child_weight': round(min_child_weight),\n",
    "    'colsample_bytree':colsample_bytree,\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this datset\n",
    "    model = xgb.train(param, dtrain)\n",
    "    preds = model.predict(dtest)\n",
    "    preds = np.asarray([np.argmax(line) for line in preds])\n",
    "    y = accuracy_score(y_test, preds)\n",
    "    x_list.append([max_depth,eta,min_child_weight,colsample_bytree])\n",
    "    y_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd5117b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888888888888888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5.162682795287844,\n",
       "  0.3629659725398362,\n",
       "  1.035207115953226,\n",
       "  0.3832880774015086]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_index(y_list):\n",
    "    index_list = []\n",
    "    for i in range(len(y_list)):\n",
    "        y = y_list[i]\n",
    "        max_value = max(y_list)\n",
    "        if y == max_value:\n",
    "            index_list.append(i)\n",
    "    print(max_value)\n",
    "    return index_list\n",
    "\n",
    "index_list = max_index(y_list)\n",
    "\n",
    "def best_x_value(x_list,index_list):\n",
    "    best_x_value_list = []\n",
    "    for i in index_list:\n",
    "        best_x_value_list.append(x_list[i])\n",
    "    return best_x_value_list\n",
    "best_x_value(x_list,index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beca8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_list = y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7db7e",
   "metadata": {},
   "source": [
    "Bayesian Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13952340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourFunction:\n",
    "    def __init__(self):\n",
    "       \n",
    "        # define the search range for each variable\n",
    "        self.bounds = np.asarray([\n",
    "                                [3.0,9.0],  # max_depth\n",
    "                                  [0.1,0.5], # eta\n",
    "                                    [1,20], #min_child_weight\n",
    "                                    [0.1,1] # colsample_bytree\n",
    "                                 ])\n",
    "            \n",
    "        self.input_dim = self.bounds.shape[0] # this is 2\n",
    "\n",
    "        # do we want to maximize the function or minimize ?\n",
    "        self.ismax=1  # set -1 if we want to minimize\n",
    "        \n",
    "        # define the known optimum value if it is available\n",
    "        self.fstar = 1 # set it None if we dont know #0.397887*self.ismax\n",
    "        \n",
    "        # define the name of your function\n",
    "        self.name='ICML20'\n",
    "        \n",
    "    def evaluate_single_fx(self,X): # this is actually a Branin function\n",
    "        # evaluate y=f(X)\n",
    "        X = np.reshape(X,self.input_dim)\n",
    "        x1,x2,x3,x4=X[0],X[1],X[2],X[3]\n",
    "        iris = datasets.load_iris()\n",
    "        X = iris.data\n",
    "        y = iris.target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "        param = {\n",
    "        'max_depth': round(x1),  # the maximum depth of each tree\n",
    "        'eta': x2,  # the training step for each iteration\n",
    "        'min_child_weight':x3,\n",
    "        'colsample_bytree':x4,\n",
    "        'silent': 1,  # logging mode - quiet\n",
    "        'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "        'num_class': 3}  # the number of classes that exist in this datset\n",
    "        model = xgb.train(param, dtrain)\n",
    "        preds = model.predict(dtest)\n",
    "        preds = np.asarray([np.argmax(line) for line in preds])\n",
    "        y = accuracy_score(y_test, preds)\n",
    "        return y*self.ismax # return the (-1) * fx for minimization problem\n",
    "   \n",
    "    \n",
    "    def func(self,X):\n",
    "        X=np.asarray(X)        \n",
    "       \n",
    "        if len(X.shape)==1: # 1 data point\n",
    "            fx=self.evaluate_single_fx(X)\n",
    "        else: # multiple data points\n",
    "            fx=np.apply_along_axis( self.evaluate_single_fx,1,X)\n",
    "            \n",
    "        return fx  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb25f7f8",
   "metadata": {},
   "source": [
    "GP & EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc7c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:33:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.88148148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.Y=(self.Y_ori-np.mean(self.Y_ori))/np.std(self.Y_ori)\n",
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:223: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  fstar_scaled=(self.fstar-np.mean(self.Y_ori))/np.std(self.Y_ori)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:33:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "     0  [ 4.449  0.409 19.513  0.226]            0.356                  0.881\n",
      "[16:33:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     1  [5.47  0.286 1.435 0.799]            0.926                  0.926\n",
      "[16:33:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:33:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     2  [5.601 0.313 6.015 0.9  ]            0.319                  0.926\n",
      "[16:34:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     3  [6.269 0.207 1.    0.878]            0.881                  0.926\n",
      "[16:34:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     4  [7.054 0.3   1.    0.794]            0.881                  0.926\n",
      "[16:34:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     5  [6.209 0.232 1.    0.616]            0.926                  0.926\n",
      "[16:34:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     6  [4.763 0.19  1.    0.713]            0.926                  0.926\n",
      "[16:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     7  [4.68  0.284 1.    0.563]            0.926                  0.926\n",
      "[16:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     8  [5.149 0.222 5.409 0.583]            0.319                  0.926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     9  [6.079 0.349 1.    0.585]            0.926                  0.926\n",
      "[16:34:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    10  [3.651 0.275 1.    0.778]            0.881                  0.926\n",
      "estimated lengthscale [0.20056708]\n",
      "[16:34:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    11  [4.542 0.384 1.    0.695]            0.926                  0.926\n",
      "[16:34:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    12  [4.721 0.388 1.    0.43 ]            0.896                  0.926\n",
      "[16:34:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    13  [4.527 0.209 1.    0.977]            0.881                  0.926\n",
      "[16:34:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    14  [3.143 0.348 1.    0.546]            0.926                  0.926\n",
      "[16:34:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    15  [3.996 0.378 5.608 0.563]            0.319                  0.926\n",
      "[16:34:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    16  [3.064 0.23  1.    0.528]            0.926                  0.926\n",
      "[16:34:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    17  [5.922 0.292 1.    0.354]            0.881                  0.926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    18  [3.742 0.292 1.    0.324]            0.881                  0.926\n",
      "[16:34:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    19  [7.594 0.289 1.    0.506]            0.926                  0.926\n"
     ]
    }
   ],
   "source": [
    "myfunction=YourFunction()\n",
    "init_X = np.asarray( [[6,0.3,1,1]])\n",
    "init_Y=myfunction.func(init_X)\n",
    "print(init_Y)\n",
    "\n",
    "# create an empty object for BO using transformed GP\n",
    "acq_name='ei'\n",
    "IsTGP=1 # using Transformed GP\n",
    "\n",
    "bo_tgp=BayesOpt_KnownOptimumValue(myfunction.func,myfunction.bounds,fstar=myfunction.fstar, \\\n",
    "                              acq_name=acq_name,IsTGP=IsTGP,verbose=1)\n",
    "bo_tgp.init_with_data(init_X=init_X,init_Y=init_Y)\n",
    "\n",
    "NN=5*myfunction.input_dim\n",
    "for index in range(0,NN):\n",
    "    bo_tgp.select_next_point()\n",
    "    #print(bo_tgp.X_ori[-1])\n",
    "    print(tabulate([[ index,np.round(bo_tgp.X_ori[-1],3), np.round(bo_tgp.Y_ori[-1],3), np.round(bo_tgp.Y_ori.max(),3)]], \\\n",
    "               headers=['Iter','Selected x', 'Output y=f(x)', 'Best Observed Value']))\n",
    "    #vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar) #一維圖-->二維故不能畫\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68fc0ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259259259259259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[5.47023565, 0.28611149, 1.43474085, 0.79881039],\n",
       "        [6.20858015, 0.23189173, 1.        , 0.61564046],\n",
       "        [4.76337802, 0.18967664, 1.        , 0.71344347],\n",
       "        [4.68013489, 0.28429937, 1.        , 0.56286395],\n",
       "        [6.07857225, 0.34875219, 1.        , 0.58541905],\n",
       "        [4.54173186, 0.38365449, 1.        , 0.69467868],\n",
       "        [3.14303579, 0.34770044, 1.        , 0.5458253 ],\n",
       "        [3.06382198, 0.22997332, 1.        , 0.52780408],\n",
       "        [7.5944277 , 0.28943303, 1.        , 0.50568256]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_index(y_list):\n",
    "    index_list = []\n",
    "    for i in range(len(y_list)):\n",
    "        y = y_list[i]\n",
    "        max_value = max(y_list)\n",
    "        if y == max_value:\n",
    "            index_list.append(i)\n",
    "    print(max_value)\n",
    "    return index_list,\n",
    "\n",
    "index_list = max_index(bo_tgp.Y_ori)\n",
    "\n",
    "def best_x_value(x_list,index_list):\n",
    "    best_x_value_list = []\n",
    "    for i in index_list:\n",
    "        best_x_value_list.append(x_list[i])\n",
    "    return best_x_value_list\n",
    "best_x_value(bo_tgp.X_ori,index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dc990ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_EI_list = list(bo_tgp.Y_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960740a4",
   "metadata": {},
   "source": [
    "TGP & EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208abcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.88148148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.Y=(self.Y_ori-np.mean(self.Y_ori))/np.std(self.Y_ori)\n",
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:223: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  fstar_scaled=(self.fstar-np.mean(self.Y_ori))/np.std(self.Y_ori)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     0  [4.905 0.499 7.398 0.516]            0.356                  0.881\n",
      "[16:34:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     1  [5.794 0.255 5.05  0.987]            0.319                  0.881\n",
      "[16:34:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     2  [6.383 0.384 1.    1.   ]            0.919                  0.919\n",
      "[16:34:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     3  [4.865 0.374 1.    1.   ]            0.919                  0.919\n",
      "[16:34:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     4  [5.661 0.359 1.    0.789]            0.881                  0.919\n",
      "[16:34:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:34:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     5  [5.515 0.428 4.725 0.913]            0.319                  0.919\n",
      "[16:35:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     6  [7.261 0.327 1.    0.841]            0.881                  0.919\n",
      "[16:35:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x               Output y=f(x)    Best Observed Value\n",
      "------  ---------------------  ---------------  ---------------------\n",
      "     7  [4.21 0.3  1.   0.85]            0.881                  0.919\n",
      "[16:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     8  [3.173 0.366 1.    1.   ]            0.881                  0.919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     9  [3.846 0.406 1.    0.779]            0.881                  0.919\n",
      "[16:35:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    10  [3.065 0.342 4.975 0.818]            0.319                  0.919\n",
      "estimated lengthscale [0.23081624]\n",
      "[16:35:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    11  [4.507 0.334 1.    0.593]            0.926                  0.926\n",
      "[16:35:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    12  [5.491 0.25  1.    0.67 ]            0.926                  0.926\n",
      "[16:35:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    13  [6.196 0.331 1.    0.526]            0.926                  0.926\n",
      "[16:35:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    14  [5.416 0.303 5.477 0.583]            0.319                  0.926\n",
      "[16:35:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    15  [7.187 0.249 1.    0.622]            0.926                  0.926\n",
      "[16:35:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    16  [6.139 0.222 1.    0.43 ]            0.859                  0.926\n",
      "[16:35:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    17  [7.858 0.354 1.    0.574]            0.926                  0.926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    18  [7.564 0.293 1.    0.369]            0.881                  0.926\n",
      "[16:35:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    19  [7.044 0.413 1.    0.385]            0.889                  0.926\n"
     ]
    }
   ],
   "source": [
    "init_X = np.asarray( [[6,0.3,1,1]])\n",
    "init_Y=myfunction.func(init_X)\n",
    "print(init_Y)\n",
    "myfunction=YourFunction()\n",
    "# create an empty object for BO using transformed GP\n",
    "acq_name='ei'\n",
    "IsTGP=1 # using Transformed GP\n",
    "\n",
    "bo_tgp=BayesOpt_KnownOptimumValue(myfunction.func,myfunction.bounds,fstar=myfunction.fstar, \\\n",
    "                              acq_name=acq_name,IsTGP=IsTGP,verbose=1)\n",
    "bo_tgp.init_with_data(init_X=init_X,init_Y=init_Y)\n",
    "\n",
    "NN=5*myfunction.input_dim\n",
    "for index in range(0,NN):\n",
    "    bo_tgp.select_next_point()\n",
    "    #print(bo_tgp.X_ori[-1])\n",
    "    print(tabulate([[ index,np.round(bo_tgp.X_ori[-1],3), np.round(bo_tgp.Y_ori[-1],3), np.round(bo_tgp.Y_ori.max(),3)]], \\\n",
    "               headers=['Iter','Selected x', 'Output y=f(x)', 'Best Observed Value']))\n",
    "    #vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar) #一維圖-->二維故不能畫\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b4b519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259259259259259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[4.50661582, 0.33434482, 1.        , 0.59294054],\n",
       "        [5.49075251, 0.25043113, 1.        , 0.66965124],\n",
       "        [6.19631063, 0.33072504, 1.        , 0.5259035 ],\n",
       "        [7.18741805, 0.24869444, 1.        , 0.62227291],\n",
       "        [7.85849182, 0.35385892, 1.        , 0.57398206]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list = max_index(bo_tgp.Y_ori)\n",
    "best_x_value(bo_tgp.X_ori,index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b26ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TGP_EI_list = list(bo_tgp.Y_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3c09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05e171f5",
   "metadata": {},
   "source": [
    "TGP & ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03cbfbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.88148148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.Y=(self.Y_ori-np.mean(self.Y_ori))/np.std(self.Y_ori)\n",
      "C:\\Users\\I-JACK.PENG\\Desktop\\BO\\2020_Knowing The What But Not The Where in Bayesian Optimization\\KnownOptimum_BO\\bayes_opt\\bo_known_optimum_value.py:223: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  fstar_scaled=(self.fstar-np.mean(self.Y_ori))/np.std(self.Y_ori)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "     0  [ 7.932  0.206 10.812  0.933]            0.356                  0.881\n",
      "erm x_max is repeated\n",
      "[16:35:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     1  [5.677 0.463 8.884 0.291]            0.356                  0.881\n",
      "[16:35:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "     2  [ 6.542  0.207 18.254  0.105]            0.356                  0.881\n",
      "[16:35:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "     3  [ 3.606  0.101 13.527  0.649]            0.356                  0.881\n",
      "[16:35:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "     4  [ 4.65   0.485 16.582  0.902]            0.356                  0.881\n",
      "[16:35:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     5  [7.667 0.127 2.526 0.242]            0.496                  0.881\n",
      "[16:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "     6  [ 8.802  0.487 19.126  0.367]            0.356                  0.881\n",
      "[16:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     7  [8.918 0.414 3.604 0.633]            0.319                  0.881\n",
      "[16:35:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "     8  [3.352 0.232 2.008 0.308]            0.496                  0.881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "     9  [ 3.74   0.408 19.436  0.165]            0.356                  0.881\n",
      "[16:35:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    10  [3.132 0.477 2.294 0.749]            0.585                  0.881\n",
      "estimated lengthscale [0.01]\n",
      "[16:35:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "    11  [ 9.     0.367 20.     0.909]            0.356                  0.881\n",
      "[16:35:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    12  [3.251 0.1   1.    0.94 ]            0.881                  0.881\n",
      "[16:35:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "    13  [ 3.     0.108 19.571  0.117]            0.356                  0.881\n",
      "[16:35:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "    14  [ 8.536  0.1   20.     0.619]            0.356                  0.881\n",
      "[16:35:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:35:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    15  [9.    0.349 9.078 0.1  ]            0.356                  0.881\n",
      "[16:36:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:36:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    16  [3.    0.294 9.722 1.   ]            0.356                  0.881\n",
      "[16:36:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:36:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                       Output y=f(x)    Best Observed Value\n",
      "------  -----------------------------  ---------------  ---------------------\n",
      "    17  [ 5.281  0.229 20.     1.   ]            0.356                  0.881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:36:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:36:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    18  [3.    0.493 1.33  0.1  ]            0.948                  0.948\n",
      "[16:36:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:36:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  Iter  Selected x                   Output y=f(x)    Best Observed Value\n",
      "------  -------------------------  ---------------  ---------------------\n",
      "    19  [9.    0.1   1.    0.794]            0.881                  0.948\n"
     ]
    }
   ],
   "source": [
    "init_X = np.asarray( [[6,0.3,1,1]])\n",
    "init_Y=myfunction.func(init_X)\n",
    "print(init_Y)\n",
    "myfunction=YourFunction()\n",
    "# create an empty object for BO using transformed GP\n",
    "acq_name='erm'\n",
    "IsTGP=1 # using Transformed GP\n",
    "\n",
    "bo_tgp=BayesOpt_KnownOptimumValue(myfunction.func,myfunction.bounds,fstar=myfunction.fstar, \\\n",
    "                              acq_name=acq_name,IsTGP=IsTGP,verbose=1)\n",
    "bo_tgp.init_with_data(init_X=init_X,init_Y=init_Y)\n",
    "\n",
    "NN=5*myfunction.input_dim\n",
    "for index in range(0,NN):\n",
    "    bo_tgp.select_next_point()\n",
    "    #print(bo_tgp.X_ori[-1])\n",
    "    print(tabulate([[ index,np.round(bo_tgp.X_ori[-1],3), np.round(bo_tgp.Y_ori[-1],3), np.round(bo_tgp.Y_ori.max(),3)]], \\\n",
    "               headers=['Iter','Selected x', 'Output y=f(x)', 'Best Observed Value']))\n",
    "    #vis_ERM.plot_1d_tgp_Forrester_EI_ERM(bo_tgp,fstar=myfunction.fstar) #一維圖-->二維故不能畫\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "431d5ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9481481481481482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.        , 0.49311546, 1.33005676, 0.1       ]])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list = max_index(bo_tgp.Y_ori)\n",
    "best_x_value(bo_tgp.X_ori,index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15ad6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "TGP_ERM_list = list(bo_tgp.Y_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1da978",
   "metadata": {},
   "source": [
    "Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12f31782",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c66dbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,random_list, color='grey',label='random')\n",
    "plt.plot(x,GP_EI_list[1:21], color='green',label='GP-EI')\n",
    "plt.plot(x,TGP_EI_list[1:21], color='orange',label='TGP-EI')\n",
    "plt.plot(x,TGP_ERM_list[1:21],color='red',label='TGP-ERM')\n",
    "plt.legend(loc = 'center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e6694",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
